{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca02780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f4a8929",
   "metadata": {},
   "source": [
    "# EasyOCR Model Training Notebook\n",
    "\n",
    "This notebook provides a comprehensive training pipeline for EasyOCR models. It includes:\n",
    "- Configuration loading from YAML files\n",
    "- Model training with customizable parameters\n",
    "- Progress monitoring and validation\n",
    "- Model checkpointing\n",
    "\n",
    "## Getting Started\n",
    "Make sure you have prepared your dataset and configuration files before running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92115e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries and modules\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Import custom modules\n",
    "from train import train\n",
    "from utils import AttrDict\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "662ca85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDNN Configuration:\n",
      "  - Benchmark: True\n",
      "  - Deterministic: False\n",
      "  - This configuration optimizes training speed but may affect reproducibility\n"
     ]
    }
   ],
   "source": [
    "# Configure CUDNN backend for performance optimization\n",
    "cudnn.benchmark = True  # Enable auto-tuner to find the best algorithm\n",
    "cudnn.deterministic = False  # Allow non-deterministic algorithms for better performance\n",
    "\n",
    "print(\"CUDNN Configuration:\")\n",
    "print(f\"  - Benchmark: {cudnn.benchmark}\")\n",
    "print(f\"  - Deterministic: {cudnn.deterministic}\")\n",
    "print(\"  - This configuration optimizes training speed but may affect reproducibility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41c56ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config(file_path):\n",
    "    \"\"\"\n",
    "    Load and process training configuration from YAML file\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the YAML configuration file\n",
    "        \n",
    "    Returns:\n",
    "        AttrDict: Configuration object with all training parameters\n",
    "    \"\"\"\n",
    "    print(f\"Loading configuration from: {file_path}\")\n",
    "    \n",
    "    # Load YAML configuration\n",
    "    with open(file_path, 'r', encoding=\"utf8\") as stream:\n",
    "        opt = yaml.safe_load(stream)\n",
    "    \n",
    "    # Convert to AttrDict for easier access\n",
    "    opt = AttrDict(opt)\n",
    "    \n",
    "    # Process character set based on configuration\n",
    "    if opt.lang_char == 'None':\n",
    "        print(\"Extracting character set from training data...\")\n",
    "        characters = ''\n",
    "        \n",
    "        # Extract characters from all selected datasets\n",
    "        for data in opt['select_data'].split('-'):\n",
    "            csv_path = os.path.join(opt['train_data'], data, 'labels.csv')\n",
    "            print(f\"  - Processing dataset: {data}\")\n",
    "            \n",
    "            # Read labels and extract unique characters\n",
    "            df = pd.read_csv(csv_path, sep='^([^,]+),', engine='python', \n",
    "                           usecols=['filename', 'words'], keep_default_na=False)\n",
    "            all_char = ''.join(df['words'])\n",
    "            characters += ''.join(set(all_char))\n",
    "        \n",
    "        # Create sorted unique character set\n",
    "        characters = sorted(set(characters))\n",
    "        opt.character = ''.join(characters)\n",
    "        print(f\"  - Extracted {len(characters)} unique characters\")\n",
    "    else:\n",
    "        # Use predefined character set\n",
    "        opt.character = opt.number + opt.symbol + opt.lang_char\n",
    "        print(f\"Using predefined character set: {len(opt.character)} characters\")\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = f'./saved_models/{opt.experiment_name}'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Models will be saved to: {output_dir}\")\n",
    "    \n",
    "    # Print key configuration parameters\n",
    "    print(\"\\nKey Configuration Parameters:\")\n",
    "    print(f\"  - Experiment name: {opt.experiment_name}\")\n",
    "    print(f\"  - Number of iterations: {opt.num_iter}\")\n",
    "    print(f\"  - Batch size: {opt.batch_size}\")\n",
    "    print(f\"  - Learning rate: {opt.lr}\")\n",
    "    print(f\"  - Image size: {opt.imgH}x{opt.imgW}\")\n",
    "    print(f\"  - Character set length: {len(opt.character)}\")\n",
    "    \n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cffb79bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Thai OCR configuration: config_files/thai_auto_config.yaml\n",
      "Loading configuration from: config_files/thai_auto_config.yaml\n",
      "Using predefined character set: 92 characters\n",
      "Models will be saved to: ./saved_models/thai_auto\n",
      "\n",
      "Key Configuration Parameters:\n",
      "  - Experiment name: thai_auto\n",
      "  - Number of iterations: 5000\n",
      "  - Batch size: 8\n",
      "  - Learning rate: 0.001\n",
      "  - Image size: 64x400\n",
      "  - Character set length: 92\n",
      "✅ Configuration loaded successfully!\n",
      "\n",
      "KEY SETTINGS:\n",
      "  Experiment: thai_auto\n",
      "  Iterations: 5,000\n",
      "  Batch size: 8\n",
      "  Learning rate: 0.001\n",
      "  Characters: 92\n"
     ]
    }
   ],
   "source": [
    "# 🇹🇭 Load Thai OCR Configuration\n",
    "config_file = 'config_files/thai_auto_config.yaml'\n",
    "print(f\"Loading Thai OCR configuration: {config_file}\")\n",
    "\n",
    "try:\n",
    "    opt = get_config(config_file)\n",
    "    print(\"✅ Configuration loaded successfully!\")\n",
    "    \n",
    "    # Show key parameters only\n",
    "    print(f\"\\nKEY SETTINGS:\")\n",
    "    print(f\"  Experiment: {opt.experiment_name}\")\n",
    "    print(f\"  Iterations: {opt.num_iter:,}\")\n",
    "    print(f\"  Batch size: {opt.batch_size}\")\n",
    "    print(f\"  Learning rate: {opt.lr}\")\n",
    "    print(f\"  Characters: {len(opt.character)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading configuration: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76ee4002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "DATASET VALIDATION\n",
      "==================================================\n",
      "🔍 Checking datasets...\n",
      "🔧 Checking for common issues...\n",
      "   No automatic fixes needed\n",
      "✅ Training data directory found: all_data\n",
      "✅ Validation uses hierarchical structure from: all_data\n",
      "\n",
      "⚠️  Found 1 issue(s):\n",
      "   ❌ Dataset not found: all_data/thai_train\n",
      "\n",
      "💡 Suggested solutions:\n",
      "   1. Check that your dataset folders contain 'labels.csv' files\n",
      "   2. Verify dataset paths in the configuration file\n",
      "   3. Make sure validation data path is correct\n",
      "   4. Consider using train_data path for validation if no separate validation set\n",
      "ℹ️  Validation: using hierarchical structure\n",
      "\n",
      "⚠️  ISSUES FOUND:\n",
      "   ❌ Missing: all_data/thai_train/labels.csv\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Dataset issues found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 171\u001b[0m\n\u001b[1;32m    169\u001b[0m     proceed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mContinue anyway? (y/n): \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m proceed\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 171\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset issues found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🎉 All checks passed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Dataset issues found"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78da8372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Checking required folders:\n",
      "❌ thai_train: Missing or no labels.csv\n",
      "❌ thai_val: Missing or no labels.csv\n",
      "\n",
      "🎯 Result: ❌ Fix folders first\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 📊 Dataset Folder Check\n",
    "print(\"📊 Checking required folders:\")\n",
    "\n",
    "# Define expected folders\n",
    "REQUIRED_FOLDERS = {\n",
    "    'thai_train': 'all_data/thai_train',\n",
    "    'thai_val': 'all_data/thai_val'\n",
    "}\n",
    "\n",
    "all_good = True\n",
    "\n",
    "for folder_name, folder_path in REQUIRED_FOLDERS.items():\n",
    "    labels_file = os.path.join(folder_path, 'labels.csv')\n",
    "    \n",
    "    if os.path.exists(labels_file):\n",
    "        try:\n",
    "            df = pd.read_csv(labels_file, sep='^([^,]+),', engine='python', \n",
    "                           usecols=['filename', 'words'], keep_default_na=False)\n",
    "            print(f\"✅ {folder_name}: OK ({len(df)} samples)\")\n",
    "        except:\n",
    "            print(f\"❌ {folder_name}: labels.csv corrupted\")\n",
    "            all_good = False\n",
    "    else:\n",
    "        print(f\"❌ {folder_name}: Missing or no labels.csv\")\n",
    "        all_good = False\n",
    "\n",
    "print(f\"\\n🎯 Result: {'✅ Ready to train!' if all_good else '❌ Fix folders first'}\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ec05f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 Quick Parameter Adjustments (Optional)\n",
    "print(\"🔧 Parameter adjustments:\")\n",
    "\n",
    "# CRITICAL PARAMETERS - Modify if needed\n",
    "ADJUSTMENTS = {\n",
    "    'batch_size': None,     # Reduce if out of memory (e.g., 4, 8, 16)\n",
    "    'num_iter': None,       # Reduce for testing (e.g., 1000, 5000)\n",
    "    'workers': None,        # Set to 0 if multiprocessing issues\n",
    "}\n",
    "\n",
    "# Apply adjustments\n",
    "modified = 0\n",
    "for param, value in ADJUSTMENTS.items():\n",
    "    if value is not None:\n",
    "        old_value = getattr(opt, param)\n",
    "        setattr(opt, param, value)\n",
    "        print(f\"   🔄 {param}: {old_value} → {value}\")\n",
    "        modified += 1\n",
    "\n",
    "if modified == 0:\n",
    "    print(\"   ✅ Using default parameters\")\n",
    "\n",
    "# Quick memory warning\n",
    "if opt.batch_size > 16:\n",
    "    print(f\"   ⚠️  Large batch_size ({opt.batch_size}) - reduce if CUDA out of memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27785150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📋 Final Training Summary\n",
    "print(\"=\"*50)\n",
    "print(\"📋 READY TO TRAIN\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"🎯 CRITICAL INFO:\")\n",
    "print(f\"   📝 Experiment: {opt.experiment_name}\")\n",
    "print(f\"   🏗️  Iterations: {opt.num_iter:,}\")\n",
    "print(f\"   📦 Batch size: {opt.batch_size}\")\n",
    "print(f\"   ⚡ Learning rate: {opt.lr}\")\n",
    "print(f\"   🔄 Validation every: {opt.valInterval} iterations\")\n",
    "\n",
    "print(f\"\\n📊 Data:\")\n",
    "print(f\"   📚 Training: {opt.select_data}\")\n",
    "print(f\"   ✅ Validation: {opt.valid_data.split('/')[-1]}\")\n",
    "\n",
    "print(f\"\\n🖼️  Image: {opt.imgH}x{opt.imgW}\")\n",
    "print(f\"🧠 Model: {opt.FeatureExtraction}+{opt.SequenceModeling}+{opt.Prediction}\")\n",
    "\n",
    "print(f\"\\n💾 Output: ./saved_models/{opt.experiment_name}/\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30132308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "STARTING MODEL TRAINING\n",
      "==================================================\n",
      "🚀 Starting training at: 2025-06-30 02:50:39\n",
      "📊 Training samples to show: 3\n",
      "⚡ Mixed precision (AMP): Disabled\n",
      "🆕 Starting training from scratch\n",
      "\n",
      "==================================================\n",
      "TRAINING LOG\n",
      "==================================================\n",
      "Filtering the images containing characters which are not in opt.character\n",
      "Filtering the images whose label is longer than opt.batch_max_length\n",
      "--------------------------------------------------------------------------------\n",
      "dataset_root: all_data\n",
      "opt.select_data: ['thai_train']\n",
      "opt.batch_ratio: ['1']\n",
      "--------------------------------------------------------------------------------\n",
      "dataset_root:    all_data\t dataset: thai_train\n",
      "\n",
      "❌ Training failed with error: datasets should not be an empty iterable\n",
      "Please check the error details above and ensure:\n",
      "1. Data paths are correct\n",
      "2. Required dependencies are installed\n",
      "3. GPU memory is sufficient\n",
      "\n",
      "🏁 Training session ended at: 2025-06-30 02:50:39\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "datasets should not be an empty iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_number\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_amp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m⚠️  Training interrupted by user (Ctrl+C)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/tmp/EasyOCR/trainer/train.py:40\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(opt, show_number, amp)\u001b[0m\n\u001b[1;32m     38\u001b[0m opt\u001b[38;5;241m.\u001b[39mselect_data \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39mselect_data\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     39\u001b[0m opt\u001b[38;5;241m.\u001b[39mbatch_ratio \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39mbatch_ratio\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mBatch_Balanced_Dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m log \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./saved_models/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mopt\u001b[38;5;241m.\u001b[39mexperiment_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/log_dataset.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m AlignCollate_valid \u001b[38;5;241m=\u001b[39m AlignCollate(imgH\u001b[38;5;241m=\u001b[39mopt\u001b[38;5;241m.\u001b[39mimgH, imgW\u001b[38;5;241m=\u001b[39mopt\u001b[38;5;241m.\u001b[39mimgW, keep_ratio_with_pad\u001b[38;5;241m=\u001b[39mopt\u001b[38;5;241m.\u001b[39mPAD, contrast_adjust\u001b[38;5;241m=\u001b[39mopt\u001b[38;5;241m.\u001b[39mcontrast_adjust)\n",
      "File \u001b[0;32m~/Desktop/tmp/EasyOCR/trainer/dataset.py:57\u001b[0m, in \u001b[0;36mBatch_Balanced_Dataset.__init__\u001b[0;34m(self, opt)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(dashed_line)\n\u001b[1;32m     56\u001b[0m log\u001b[38;5;241m.\u001b[39mwrite(dashed_line \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 57\u001b[0m _dataset, _dataset_log \u001b[38;5;241m=\u001b[39m \u001b[43mhierarchical_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselect_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mselected_d\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m total_number_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(_dataset)\n\u001b[1;32m     59\u001b[0m log\u001b[38;5;241m.\u001b[39mwrite(_dataset_log)\n",
      "File \u001b[0;32m~/Desktop/tmp/EasyOCR/trainer/dataset.py:139\u001b[0m, in \u001b[0;36mhierarchical_dataset\u001b[0;34m(root, opt, select_data)\u001b[0m\n\u001b[1;32m    136\u001b[0m             dataset_log \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msub_dataset_log\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    137\u001b[0m             dataset_list\u001b[38;5;241m.\u001b[39mappend(dataset)\n\u001b[0;32m--> 139\u001b[0m concatenated_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mConcatDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concatenated_dataset, dataset_log\n",
      "File \u001b[0;32m~/Desktop/tmp/EasyOCR/.venv/lib/python3.10/site-packages/torch/utils/data/dataset.py:324\u001b[0m, in \u001b[0;36mConcatDataset.__init__\u001b[0;34m(self, datasets)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatasets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(datasets)\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatasets) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets should not be an empty iterable\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatasets:\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    327\u001b[0m         d, IterableDataset\n\u001b[1;32m    328\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConcatDataset does not support IterableDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: datasets should not be an empty iterable"
     ]
    }
   ],
   "source": [
    "# Start model training\n",
    "print(\"=\"*50)\n",
    "print(\"STARTING MODEL TRAINING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Training parameters\n",
    "use_amp = False  # Set to True to enable Automatic Mixed Precision for faster training\n",
    "show_samples = 3  # Number of prediction samples to show during validation\n",
    "\n",
    "print(f\"🚀 Starting training at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"📊 Training samples to show: {show_samples}\")\n",
    "print(f\"⚡ Mixed precision (AMP): {'Enabled' if use_amp else 'Disabled'}\")\n",
    "\n",
    "# Check if resuming from checkpoint\n",
    "if opt.saved_model and opt.saved_model != '':\n",
    "    print(f\"🔄 Resuming training from: {opt.saved_model}\")\n",
    "else:\n",
    "    print(\"🆕 Starting training from scratch\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING LOG\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    # Start training\n",
    "    train(opt, show_number=show_samples, amp=use_amp)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n⚠️  Training interrupted by user (Ctrl+C)\")\n",
    "    print(\"Model checkpoints are saved in: ./saved_models/{}/\".format(opt.experiment_name))\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Training failed with error: {e}\")\n",
    "    print(\"Please check the error details above and ensure:\")\n",
    "    print(\"1. Data paths are correct\")\n",
    "    print(\"2. Required dependencies are installed\")\n",
    "    print(\"3. GPU memory is sufficient\")\n",
    "    raise\n",
    "finally:\n",
    "    print(f\"\\n🏁 Training session ended at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ba44b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Quick Monitoring Tools\n",
    "print(\"📊 Monitoring tools ready:\")\n",
    "\n",
    "def check_progress(experiment_name):\n",
    "    \"\"\"Quick progress check\"\"\"\n",
    "    log_dir = f\"./saved_models/{experiment_name}\"\n",
    "    \n",
    "    if not os.path.exists(log_dir):\n",
    "        print(f\"❌ No logs yet: {log_dir}\")\n",
    "        return\n",
    "    \n",
    "    # Check models\n",
    "    models = [f for f in os.listdir(log_dir) if f.endswith('.pth')]\n",
    "    if models:\n",
    "        print(f\"💾 Models: {len(models)} saved\")\n",
    "        for model in sorted(models)[-3:]:  # Show last 3\n",
    "            size_mb = os.path.getsize(os.path.join(log_dir, model)) / (1024*1024)\n",
    "            print(f\"   - {model} ({size_mb:.1f}MB)\")\n",
    "    \n",
    "    # Check training log\n",
    "    log_file = os.path.join(log_dir, \"log_train.txt\")\n",
    "    if os.path.exists(log_file):\n",
    "        with open(log_file, 'r', encoding='utf8') as f:\n",
    "            lines = f.readlines()\n",
    "        print(f\"📄 Training log: {len(lines)} lines\")\n",
    "        if lines:\n",
    "            print(f\"   Last: {lines[-1].strip()}\")\n",
    "\n",
    "def quick_log(experiment_name, lines=5):\n",
    "    \"\"\"Show last few log lines\"\"\"\n",
    "    log_file = f\"./saved_models/{experiment_name}/log_train.txt\"\n",
    "    if os.path.exists(log_file):\n",
    "        with open(log_file, 'r', encoding='utf8') as f:\n",
    "            log_lines = f.readlines()\n",
    "        print(f\"📄 Last {lines} lines:\")\n",
    "        for line in log_lines[-lines:]:\n",
    "            print(f\"   {line.strip()}\")\n",
    "    else:\n",
    "        print(\"❌ No training log found\")\n",
    "\n",
    "print(\"   📊 check_progress('experiment_name') - Check saved models\")\n",
    "print(\"   📄 quick_log('experiment_name') - Show recent logs\")\n",
    "print(\"   Example: check_progress(opt.experiment_name)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadf79c5",
   "metadata": {},
   "source": [
    "## 💡 Essential Tips\n",
    "\n",
    "### 🚨 Critical Issues:\n",
    "- **Out of memory**: Reduce `batch_size` in cell 8\n",
    "- **Slow training**: Set `workers=0` if multiprocessing issues\n",
    "- **Stop training**: Ctrl+C (models auto-saved every 10k iterations)\n",
    "\n",
    "### 📁 After Training:\n",
    "Check `./saved_models/{experiment_name}/`:\n",
    "- `best_accuracy.pth` - Best model\n",
    "- `log_train.txt` - Training progress\n",
    "\n",
    "### 🔧 Quick Fixes:\n",
    "- Memory error → Reduce batch_size to 4 or 8\n",
    "- File not found → Check dataset paths in config file\n",
    "- Training stuck → Set workers=0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "easyocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
