{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f4a8929",
   "metadata": {},
   "source": [
    "# EasyOCR Model Training Notebook\n",
    "\n",
    "This notebook provides a comprehensive training pipeline for EasyOCR models. It includes:\n",
    "- Configuration loading from YAML files\n",
    "- Model training with customizable parameters\n",
    "- Progress monitoring and validation\n",
    "- Model checkpointing\n",
    "\n",
    "## Getting Started\n",
    "Make sure you have prepared your dataset and configuration files before running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92115e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries and modules\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Import custom modules\n",
    "from train import train\n",
    "from utils import AttrDict\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "662ca85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDNN Configuration:\n",
      "  - Benchmark: True\n",
      "  - Deterministic: False\n",
      "  - This configuration optimizes training speed but may affect reproducibility\n"
     ]
    }
   ],
   "source": [
    "# Configure CUDNN backend for performance optimization\n",
    "cudnn.benchmark = True  # Enable auto-tuner to find the best algorithm\n",
    "cudnn.deterministic = False  # Allow non-deterministic algorithms for better performance\n",
    "\n",
    "print(\"CUDNN Configuration:\")\n",
    "print(f\"  - Benchmark: {cudnn.benchmark}\")\n",
    "print(f\"  - Deterministic: {cudnn.deterministic}\")\n",
    "print(\"  - This configuration optimizes training speed but may affect reproducibility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41c56ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config(file_path):\n",
    "    \"\"\"\n",
    "    Load and process training configuration from YAML file\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the YAML configuration file\n",
    "        \n",
    "    Returns:\n",
    "        AttrDict: Configuration object with all training parameters\n",
    "    \"\"\"\n",
    "    print(f\"Loading configuration from: {file_path}\")\n",
    "    \n",
    "    # Load YAML configuration\n",
    "    with open(file_path, 'r', encoding=\"utf8\") as stream:\n",
    "        opt = yaml.safe_load(stream)\n",
    "    \n",
    "    # Convert to AttrDict for easier access\n",
    "    opt = AttrDict(opt)\n",
    "    \n",
    "    # Process character set based on configuration\n",
    "    if opt.lang_char == 'None':\n",
    "        print(\"Extracting character set from training data...\")\n",
    "        characters = ''\n",
    "        \n",
    "        # Extract characters from all selected datasets\n",
    "        for data in opt['select_data'].split('-'):\n",
    "            csv_path = os.path.join(opt['train_data'], data, 'labels.csv')\n",
    "            print(f\"  - Processing dataset: {data}\")\n",
    "            \n",
    "            # Read labels and extract unique characters\n",
    "            df = pd.read_csv(csv_path, sep='^([^,]+),', engine='python', \n",
    "                           usecols=['filename', 'words'], keep_default_na=False)\n",
    "            all_char = ''.join(df['words'])\n",
    "            characters += ''.join(set(all_char))\n",
    "        \n",
    "        # Create sorted unique character set\n",
    "        characters = sorted(set(characters))\n",
    "        opt.character = ''.join(characters)\n",
    "        print(f\"  - Extracted {len(characters)} unique characters\")\n",
    "    else:\n",
    "        # Use predefined character set\n",
    "        opt.character = opt.number + opt.symbol + opt.lang_char\n",
    "        print(f\"Using predefined character set: {len(opt.character)} characters\")\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = f'./saved_models/{opt.experiment_name}'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Models will be saved to: {output_dir}\")\n",
    "    \n",
    "    # Print key configuration parameters\n",
    "    print(\"\\nKey Configuration Parameters:\")\n",
    "    print(f\"  - Experiment name: {opt.experiment_name}\")\n",
    "    print(f\"  - Number of iterations: {opt.num_iter}\")\n",
    "    print(f\"  - Batch size: {opt.batch_size}\")\n",
    "    print(f\"  - Learning rate: {opt.lr}\")\n",
    "    print(f\"  - Image size: {opt.imgH}x{opt.imgW}\")\n",
    "    print(f\"  - Character set length: {len(opt.character)}\")\n",
    "    \n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cffb79bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "AVAILABLE CONFIGURATIONS\n",
      "==================================================\n",
      "üìÑ en_filtered_config: en_filtered_config.yaml\n",
      "üìÑ thai_auto_config: thai_auto_config.yaml\n",
      "\n",
      "==================================================\n",
      "LOADING TRAINING CONFIGURATION\n",
      "==================================================\n",
      "üéØ Selected Configuration: thai\n",
      "üìÅ Config file: config_files/thai_auto_config.yaml\n",
      "Loading configuration from: config_files/thai_auto_config.yaml\n",
      "Using predefined character set: 92 characters\n",
      "Models will be saved to: ./saved_models/thai_auto\n",
      "\n",
      "Key Configuration Parameters:\n",
      "  - Experiment name: thai_auto\n",
      "  - Number of iterations: 5000\n",
      "  - Batch size: 8\n",
      "  - Learning rate: 0.001\n",
      "  - Image size: 64x400\n",
      "  - Character set length: 92\n",
      "\n",
      "‚úÖ Configuration loaded successfully!\n",
      "üöÄ Training will start with experiment: 'thai_auto'\n",
      "\n",
      "üìä Dataset Information:\n",
      "   - Training data directory: all_data\n",
      "   - Validation data directory: all_data/thai_val\n",
      "   - Selected datasets: thai_train\n",
      "   - Batch ratios: 1\n"
     ]
    }
   ],
   "source": [
    "# Load training configuration\n",
    "print(\"=\"*50)\n",
    "print(\"AVAILABLE CONFIGURATIONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check available configurations\n",
    "config_dir = \"config_files\"\n",
    "available_configs = {}\n",
    "\n",
    "if os.path.exists(config_dir):\n",
    "    for file in os.listdir(config_dir):\n",
    "        if file.endswith(\".yaml\") or file.endswith(\".yml\"):\n",
    "            config_name = file.replace('.yaml', '').replace('.yml', '')\n",
    "            available_configs[config_name] = f\"{config_dir}/{file}\"\n",
    "            print(f\"üìÑ {config_name}: {file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LOADING TRAINING CONFIGURATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Select configuration - Change this to use your preferred config\n",
    "CONFIG_OPTIONS = {\n",
    "    'thai': 'thai_auto_config.yaml',     # Thai language dataset\n",
    "    'english': 'en_filtered_config.yaml', # English language dataset  \n",
    "    'custom': 'custom_config.yaml'       # Create your own config\n",
    "}\n",
    "\n",
    "# üîß CONFIGURATION SELECTION - Modify this line to change dataset\n",
    "selected_config = 'thai'  # Options: 'thai', 'english', or 'custom'\n",
    "\n",
    "config_file = f\"config_files/{CONFIG_OPTIONS[selected_config]}\"\n",
    "\n",
    "print(f\"üéØ Selected Configuration: {selected_config}\")\n",
    "print(f\"üìÅ Config file: {config_file}\")\n",
    "\n",
    "try:\n",
    "    opt = get_config(config_file)\n",
    "    print(f\"\\n‚úÖ Configuration loaded successfully!\")\n",
    "    print(f\"üöÄ Training will start with experiment: '{opt.experiment_name}'\")\n",
    "    \n",
    "    # Display detected datasets\n",
    "    print(f\"\\nüìä Dataset Information:\")\n",
    "    print(f\"   - Training data directory: {opt.train_data}\")\n",
    "    print(f\"   - Validation data directory: {opt.valid_data}\")\n",
    "    print(f\"   - Selected datasets: {opt.select_data}\")\n",
    "    print(f\"   - Batch ratios: {opt.batch_ratio}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Error: Configuration file '{config_file}' not found!\")\n",
    "    print(\"Please make sure the config file exists and the path is correct.\")\n",
    "    print(f\"\\nAvailable config files in {config_dir}:\")\n",
    "    for name, path in available_configs.items():\n",
    "        print(f\"  - {name}: {path}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading configuration: {e}\")\n",
    "    print(\"Please check the configuration file format and content.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76ee4002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "DATASET VALIDATION\n",
      "==================================================\n",
      "üîß Checking for common issues...\n",
      "   No automatic fixes needed\n",
      "‚úÖ Training data directory found: all_data\n",
      "‚úÖ Dataset 'thai_train': 80 samples\n",
      "‚úÖ Validation data directory found: all_data/thai_val\n",
      "‚úÖ Validation dataset: 20 samples\n",
      "\n",
      "üéâ All dataset validations passed!\n",
      "Ready to proceed with training.\n"
     ]
    }
   ],
   "source": [
    "# Validate dataset structure\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATASET VALIDATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def validate_dataset_structure(opt):\n",
    "    \"\"\"Validate that required datasets and files exist\"\"\"\n",
    "    issues = []\n",
    "    \n",
    "    # Check train data directory\n",
    "    if not os.path.exists(opt.train_data):\n",
    "        issues.append(f\"‚ùå Training data directory not found: {opt.train_data}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Training data directory found: {opt.train_data}\")\n",
    "        \n",
    "        # Check selected datasets\n",
    "        selected_datasets = opt.select_data.split('-')\n",
    "        for dataset in selected_datasets:\n",
    "            dataset_path = os.path.join(opt.train_data, dataset)\n",
    "            if not os.path.exists(dataset_path):\n",
    "                issues.append(f\"‚ùå Dataset not found: {dataset_path}\")\n",
    "            else:\n",
    "                # Check for labels.csv\n",
    "                labels_file = os.path.join(dataset_path, 'labels.csv')\n",
    "                if not os.path.exists(labels_file):\n",
    "                    issues.append(f\"‚ùå Labels file not found: {labels_file}\")\n",
    "                else:\n",
    "                    # Count samples\n",
    "                    try:\n",
    "                        df = pd.read_csv(labels_file, sep='^([^,]+),', engine='python', \n",
    "                                       usecols=['filename', 'words'], keep_default_na=False)\n",
    "                        sample_count = len(df)\n",
    "                        print(f\"‚úÖ Dataset '{dataset}': {sample_count} samples\")\n",
    "                    except Exception as e:\n",
    "                        issues.append(f\"‚ùå Error reading {labels_file}: {e}\")\n",
    "    \n",
    "    # Check validation data\n",
    "    if hasattr(opt, 'valid_data') and opt.valid_data:\n",
    "        # Handle different validation data configurations\n",
    "        if opt.valid_data == opt.train_data:\n",
    "            # If valid_data same as train_data, it will use hierarchical structure\n",
    "            print(f\"‚úÖ Validation uses hierarchical structure from: {opt.valid_data}\")\n",
    "        else:\n",
    "            # Check specific validation directory\n",
    "            if not os.path.exists(opt.valid_data):\n",
    "                issues.append(f\"‚ùå Validation data directory not found: {opt.valid_data}\")\n",
    "            else:\n",
    "                print(f\"‚úÖ Validation data directory found: {opt.valid_data}\")\n",
    "                # Check if it has labels.csv (for direct validation dataset)\n",
    "                val_labels_file = os.path.join(opt.valid_data, 'labels.csv')\n",
    "                if os.path.exists(val_labels_file):\n",
    "                    try:\n",
    "                        df = pd.read_csv(val_labels_file, sep='^([^,]+),', engine='python', \n",
    "                                       usecols=['filename', 'words'], keep_default_na=False)\n",
    "                        val_sample_count = len(df)\n",
    "                        print(f\"‚úÖ Validation dataset: {val_sample_count} samples\")\n",
    "                    except Exception as e:\n",
    "                        issues.append(f\"‚ùå Error reading {val_labels_file}: {e}\")\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è  No direct labels.csv in validation directory\")\n",
    "                    print(f\"   Validation will use hierarchical structure\")\n",
    "    \n",
    "    return issues\n",
    "\n",
    "def fix_common_issues(opt):\n",
    "    \"\"\"Automatically fix common configuration issues\"\"\"\n",
    "    fixes_applied = []\n",
    "    \n",
    "    # Check if validation data points to non-existent path and try to fix\n",
    "    if hasattr(opt, 'valid_data') and opt.valid_data:\n",
    "        if not os.path.exists(opt.valid_data):\n",
    "            # Try common validation directory patterns\n",
    "            potential_dirs = [\n",
    "                f\"{opt.train_data}/val\",\n",
    "                f\"{opt.train_data}/validation\", \n",
    "                f\"{opt.train_data}/thai_val\",\n",
    "                f\"{opt.train_data}/en_val\",\n",
    "                opt.train_data  # Use same as training data for hierarchical\n",
    "            ]\n",
    "            \n",
    "            for potential_dir in potential_dirs:\n",
    "                if os.path.exists(potential_dir):\n",
    "                    original_path = opt.valid_data\n",
    "                    opt.valid_data = potential_dir\n",
    "                    fixes_applied.append(f\"üîß Changed valid_data: {original_path} ‚Üí {potential_dir}\")\n",
    "                    break\n",
    "    \n",
    "    return fixes_applied\n",
    "\n",
    "# Apply automatic fixes first\n",
    "print(\"üîß Checking for common issues...\")\n",
    "fixes = fix_common_issues(opt)\n",
    "if fixes:\n",
    "    print(\"Applied automatic fixes:\")\n",
    "    for fix in fixes:\n",
    "        print(f\"   {fix}\")\n",
    "else:\n",
    "    print(\"   No automatic fixes needed\")\n",
    "\n",
    "# Run validation\n",
    "validation_issues = validate_dataset_structure(opt)\n",
    "\n",
    "if validation_issues:\n",
    "    print(f\"\\n‚ö†Ô∏è  Found {len(validation_issues)} issue(s):\")\n",
    "    for issue in validation_issues:\n",
    "        print(f\"   {issue}\")\n",
    "    \n",
    "    # Suggest solutions\n",
    "    print(f\"\\nüí° Suggested solutions:\")\n",
    "    print(f\"   1. Check that your dataset folders contain 'labels.csv' files\")\n",
    "    print(f\"   2. Verify dataset paths in the configuration file\")\n",
    "    print(f\"   3. Make sure validation data path is correct\")\n",
    "    print(f\"   4. Consider using train_data path for validation if no separate validation set\")\n",
    "    \n",
    "    # Don't stop here, let user decide\n",
    "    user_continue = input(\"\\nDo you want to continue training anyway? (y/n): \")\n",
    "    if user_continue.lower() != 'y':\n",
    "        print(\"Training stopped. Please fix the issues above.\")\n",
    "        raise Exception(\"Dataset validation failed\")\n",
    "else:\n",
    "    print(f\"\\nüéâ All dataset validations passed!\")\n",
    "    print(\"Ready to proceed with training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78da8372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "DATASET STRUCTURE OVERVIEW\n",
      "==================================================\n",
      "üìÅ Dataset structure under 'all_data':\n",
      "‚îú‚îÄ‚îÄ üìÅ easy_ocr/\n",
      "‚îÇ   ‚îî‚îÄ‚îÄ üìÑ custom_example.yaml (227 bytes)\n",
      "‚îú‚îÄ‚îÄ üìÅ results_JS-Kobori/\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ üìÅ glyph_masks/\n",
      "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÅ 0/\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ üìÅ images/\n",
      "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ 0/\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ üìÅ masks/\n",
      "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÅ 0/\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ üìÑ coords.txt (14609 bytes)\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ üìÑ glyph_coords.txt (14609 bytes)\n",
      "‚îÇ   ‚îî‚îÄ‚îÄ üìÑ gt.txt (5588 bytes)\n",
      "‚îú‚îÄ‚îÄ üìÅ thai_train/\n",
      "‚îÇ   ‚îî‚îÄ‚îÄ üìÑ labels.csv (80 samples)\n",
      "‚îÇ   ‚îî‚îÄ‚îÄ üìÑ labels.csv (3719 bytes)\n",
      "‚îî‚îÄ‚îÄ üìÅ thai_val/\n",
      "    ‚îî‚îÄ‚îÄ üìÑ labels.csv (20 samples)\n",
      "    ‚îî‚îÄ‚îÄ üìÑ labels.csv (1001 bytes)\n",
      "\n",
      "üéØ Current Configuration:\n",
      "   - Training data: all_data\n",
      "   - Validation data: all_data/thai_val\n",
      "   - Selected training datasets: thai_train\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# üìä Dataset Structure Overview\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATASET STRUCTURE OVERVIEW\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def show_dataset_structure(base_path=\"all_data\", max_depth=3):\n",
    "    \"\"\"Show the structure of datasets\"\"\"\n",
    "    if not os.path.exists(base_path):\n",
    "        print(f\"‚ùå Base path not found: {base_path}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üìÅ Dataset structure under '{base_path}':\")\n",
    "    \n",
    "    def print_tree(path, prefix=\"\", depth=0):\n",
    "        if depth >= max_depth:\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            items = sorted(os.listdir(path))\n",
    "            dirs = [item for item in items if os.path.isdir(os.path.join(path, item))]\n",
    "            files = [item for item in items if os.path.isfile(os.path.join(path, item))]\n",
    "            \n",
    "            # Print directories first\n",
    "            for i, dir_name in enumerate(dirs):\n",
    "                is_last_dir = (i == len(dirs) - 1) and len(files) == 0\n",
    "                current_prefix = \"‚îî‚îÄ‚îÄ \" if is_last_dir else \"‚îú‚îÄ‚îÄ \"\n",
    "                print(f\"{prefix}{current_prefix}üìÅ {dir_name}/\")\n",
    "                \n",
    "                # Check if it has labels.csv\n",
    "                labels_path = os.path.join(path, dir_name, \"labels.csv\")\n",
    "                if os.path.exists(labels_path):\n",
    "                    try:\n",
    "                        df = pd.read_csv(labels_path, sep='^([^,]+),', engine='python', \n",
    "                                       usecols=['filename', 'words'], keep_default_na=False)\n",
    "                        sample_count = len(df)\n",
    "                        next_prefix = prefix + (\"    \" if is_last_dir else \"‚îÇ   \")\n",
    "                        print(f\"{next_prefix}‚îî‚îÄ‚îÄ üìÑ labels.csv ({sample_count} samples)\")\n",
    "                    except:\n",
    "                        next_prefix = prefix + (\"    \" if is_last_dir else \"‚îÇ   \")\n",
    "                        print(f\"{next_prefix}‚îî‚îÄ‚îÄ üìÑ labels.csv (error reading)\")\n",
    "                \n",
    "                # Recurse into subdirectory\n",
    "                next_prefix = prefix + (\"    \" if is_last_dir else \"‚îÇ   \")\n",
    "                print_tree(os.path.join(path, dir_name), next_prefix, depth + 1)\n",
    "            \n",
    "            # Print key files\n",
    "            key_files = [f for f in files if f.endswith(('.csv', '.txt', '.yaml', '.yml'))]\n",
    "            for i, file_name in enumerate(key_files):\n",
    "                is_last = i == len(key_files) - 1\n",
    "                current_prefix = \"‚îî‚îÄ‚îÄ \" if is_last else \"‚îú‚îÄ‚îÄ \"\n",
    "                file_path = os.path.join(path, file_name)\n",
    "                file_size = os.path.getsize(file_path)\n",
    "                print(f\"{prefix}{current_prefix}üìÑ {file_name} ({file_size} bytes)\")\n",
    "                \n",
    "        except PermissionError:\n",
    "            print(f\"{prefix}‚îî‚îÄ‚îÄ ‚ùå Permission denied\")\n",
    "        except Exception as e:\n",
    "            print(f\"{prefix}‚îî‚îÄ‚îÄ ‚ùå Error: {e}\")\n",
    "    \n",
    "    print_tree(base_path)\n",
    "\n",
    "# Show current dataset structure\n",
    "show_dataset_structure()\n",
    "\n",
    "print(f\"\\nüéØ Current Configuration:\")\n",
    "print(f\"   - Training data: {opt.train_data}\")\n",
    "print(f\"   - Validation data: {opt.valid_data}\")\n",
    "print(f\"   - Selected training datasets: {opt.select_data}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3ec05f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TRAINING PARAMETER CUSTOMIZATION\n",
      "==================================================\n",
      "üìù Current parameters (you can modify these):\n",
      "   ‚úÖ Using original config parameters\n",
      "\n",
      "üñ•Ô∏è  Memory Optimization Suggestions:\n",
      "   Current batch_size: 8\n",
      "   üí° If you get CUDA out of memory errors, try reducing batch_size to 4\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# üîß Optional: Customize training parameters\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING PARAMETER CUSTOMIZATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# You can modify these parameters without changing the config file\n",
    "print(\"üìù Current parameters (you can modify these):\")\n",
    "\n",
    "# Training parameters you might want to adjust\n",
    "CUSTOM_PARAMS = {\n",
    "    'batch_size': None,        # Set to override config (e.g., 16, 32, 64)\n",
    "    'num_iter': None,          # Set to override config (e.g., 10000, 50000, 100000)\n",
    "    'lr': None,                # Set to override config (e.g., 0.001, 0.0001)\n",
    "    'valInterval': None,       # Set to override config (e.g., 1000, 5000)\n",
    "    'workers': None,           # Set to override config (e.g., 0, 2, 4)\n",
    "    'imgH': None,              # Set to override config (e.g., 32, 64)\n",
    "    'imgW': None,              # Set to override config (e.g., 200, 400, 600)\n",
    "}\n",
    "\n",
    "# Apply custom parameters\n",
    "original_params = {}\n",
    "for param, value in CUSTOM_PARAMS.items():\n",
    "    if value is not None and hasattr(opt, param):\n",
    "        original_params[param] = getattr(opt, param)\n",
    "        setattr(opt, param, value)\n",
    "        print(f\"   üîÑ {param}: {original_params[param]} ‚Üí {value}\")\n",
    "\n",
    "if not original_params:\n",
    "    print(\"   ‚úÖ Using original config parameters\")\n",
    "else:\n",
    "    print(f\"   üìä Modified {len(original_params)} parameters\")\n",
    "\n",
    "# Memory optimization for different GPU sizes\n",
    "print(f\"\\nüñ•Ô∏è  Memory Optimization Suggestions:\")\n",
    "print(f\"   Current batch_size: {opt.batch_size}\")\n",
    "\n",
    "gpu_memory_gb = None  # Set this if you know your GPU memory\n",
    "if gpu_memory_gb:\n",
    "    if gpu_memory_gb >= 16:\n",
    "        suggested_batch = min(64, opt.batch_size)\n",
    "        print(f\"   üí™ High-end GPU ({gpu_memory_gb}GB): Consider batch_size up to {suggested_batch}\")\n",
    "    elif gpu_memory_gb >= 8:\n",
    "        suggested_batch = min(32, opt.batch_size)\n",
    "        print(f\"   üéØ Mid-range GPU ({gpu_memory_gb}GB): Consider batch_size up to {suggested_batch}\")\n",
    "    else:\n",
    "        suggested_batch = min(16, opt.batch_size)\n",
    "        print(f\"   ‚ö° Lower-end GPU ({gpu_memory_gb}GB): Consider batch_size up to {suggested_batch}\")\n",
    "else:\n",
    "    print(f\"   üí° If you get CUDA out of memory errors, try reducing batch_size to {opt.batch_size // 2}\")\n",
    "\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27785150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "TRAINING CONFIGURATION SUMMARY\n",
      "==================================================\n",
      "üìÅ Data Configuration:\n",
      "   - Training data: all_data\n",
      "   - Validation data: all_data/thai_val\n",
      "   - Selected datasets: thai_train\n",
      "   - Batch ratios: 1\n",
      "\n",
      "üñºÔ∏è  Image Configuration:\n",
      "   - Image height: 64\n",
      "   - Image width: 400\n",
      "   - RGB mode: False\n",
      "   - Padding: True\n",
      "\n",
      "üß† Model Configuration:\n",
      "   - Transformation: None\n",
      "   - Feature extraction: VGG\n",
      "   - Sequence modeling: BiLSTM\n",
      "   - Prediction: CTC\n",
      "\n",
      "‚öôÔ∏è  Training Configuration:\n",
      "   - Batch size: 8\n",
      "   - Number of iterations: 5000\n",
      "   - Learning rate: 0.001\n",
      "   - Optimizer: adam\n",
      "   - Validation interval: 250\n",
      "   - Number of workers: 0\n",
      "\n",
      "üíæ Output Configuration:\n",
      "   - Experiment name: thai_auto\n",
      "   - Save directory: ./saved_models/thai_auto\n",
      "\n",
      "üéØ Training Options:\n",
      "   - Mixed precision (AMP): Disabled\n",
      "   - Data filtering: Enabled\n",
      "   - Fine-tuning: Disabled\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Display training options and parameters\n",
    "print(\"=\"*50)\n",
    "print(\"TRAINING CONFIGURATION SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"üìÅ Data Configuration:\")\n",
    "print(f\"   - Training data: {opt.train_data}\")\n",
    "print(f\"   - Validation data: {opt.valid_data}\")\n",
    "print(f\"   - Selected datasets: {opt.select_data}\")\n",
    "print(f\"   - Batch ratios: {opt.batch_ratio}\")\n",
    "\n",
    "print(f\"\\nüñºÔ∏è  Image Configuration:\")\n",
    "print(f\"   - Image height: {opt.imgH}\")\n",
    "print(f\"   - Image width: {opt.imgW}\")\n",
    "print(f\"   - RGB mode: {opt.rgb}\")\n",
    "print(f\"   - Padding: {opt.PAD}\")\n",
    "\n",
    "print(f\"\\nüß† Model Configuration:\")\n",
    "print(f\"   - Transformation: {opt.Transformation}\")\n",
    "print(f\"   - Feature extraction: {opt.FeatureExtraction}\")\n",
    "print(f\"   - Sequence modeling: {opt.SequenceModeling}\")\n",
    "print(f\"   - Prediction: {opt.Prediction}\")\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è  Training Configuration:\")\n",
    "print(f\"   - Batch size: {opt.batch_size}\")\n",
    "print(f\"   - Number of iterations: {opt.num_iter}\")\n",
    "print(f\"   - Learning rate: {opt.lr}\")\n",
    "print(f\"   - Optimizer: {opt.optim}\")\n",
    "print(f\"   - Validation interval: {opt.valInterval}\")\n",
    "print(f\"   - Number of workers: {opt.workers}\")\n",
    "\n",
    "print(f\"\\nüíæ Output Configuration:\")\n",
    "print(f\"   - Experiment name: {opt.experiment_name}\")\n",
    "print(f\"   - Save directory: ./saved_models/{opt.experiment_name}\")\n",
    "\n",
    "# Training options\n",
    "print(f\"\\nüéØ Training Options:\")\n",
    "print(f\"   - Mixed precision (AMP): {'Enabled' if hasattr(opt, 'amp') and opt.amp else 'Disabled'}\")\n",
    "print(f\"   - Data filtering: {'Disabled' if opt.data_filtering_off else 'Enabled'}\")\n",
    "print(f\"   - Fine-tuning: {'Enabled' if opt.FT else 'Disabled'}\")\n",
    "\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30132308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "STARTING MODEL TRAINING\n",
      "==================================================\n",
      "üöÄ Starting training at: 2025-06-28 01:42:35\n",
      "üìä Training samples to show: 3\n",
      "‚ö° Mixed precision (AMP): Disabled\n",
      "üÜï Starting training from scratch\n",
      "\n",
      "==================================================\n",
      "TRAINING LOG\n",
      "==================================================\n",
      "Filtering the images containing characters which are not in opt.character\n",
      "Filtering the images whose label is longer than opt.batch_max_length\n",
      "--------------------------------------------------------------------------------\n",
      "dataset_root: all_data\n",
      "opt.select_data: ['thai_train']\n",
      "opt.batch_ratio: ['1']\n",
      "--------------------------------------------------------------------------------\n",
      "dataset_root:    all_data\t dataset: thai_train\n",
      "all_data/thai_train\n",
      "sub-directory:\t/thai_train\t num samples: 80\n",
      "num total samples of thai_train: 80 x 1.0 (total_data_usage_ratio) = 80\n",
      "num samples of thai_train per batch: 8 x 1.0 (batch_ratio) = 8\n",
      "--------------------------------------------------------------------------------\n",
      "Total_batch_size: 8 = 8\n",
      "--------------------------------------------------------------------------------\n",
      "dataset_root:    all_data/thai_val\t dataset: /\n",
      "all_data/thai_val/\n",
      "sub-directory:\t/.\t num samples: 20\n",
      "--------------------------------------------------------------------------------\n",
      "No Transformation module specified\n",
      "model input parameters 64 400 20 1 256 256 93 30 None VGG BiLSTM CTC\n",
      "Model:\n",
      "DataParallel(\n",
      "  (module): Model(\n",
      "    (FeatureExtraction): VGG_FeatureExtractor(\n",
      "      (ConvNet): Sequential(\n",
      "        (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): ReLU(inplace=True)\n",
      "        (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (7): ReLU(inplace=True)\n",
      "        (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (9): ReLU(inplace=True)\n",
      "        (10): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "        (11): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (13): ReLU(inplace=True)\n",
      "        (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (16): ReLU(inplace=True)\n",
      "        (17): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "        (18): Conv2d(256, 256, kernel_size=(2, 2), stride=(1, 1))\n",
      "        (19): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (AdaptiveAvgPool): AdaptiveAvgPool2d(output_size=(None, 1))\n",
      "    (SequenceModeling): Sequential(\n",
      "      (0): BidirectionalLSTM(\n",
      "        (rnn): LSTM(256, 256, batch_first=True, bidirectional=True)\n",
      "        (linear): Linear(in_features=512, out_features=256, bias=True)\n",
      "      )\n",
      "      (1): BidirectionalLSTM(\n",
      "        (rnn): LSTM(256, 256, batch_first=True, bidirectional=True)\n",
      "        (linear): Linear(in_features=512, out_features=256, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (Prediction): Linear(in_features=256, out_features=93, bias=True)\n",
      "  )\n",
      ")\n",
      "Modules, Parameters\n",
      "module.FeatureExtraction.ConvNet.0.weight 288\n",
      "module.FeatureExtraction.ConvNet.0.bias 32\n",
      "module.FeatureExtraction.ConvNet.3.weight 18432\n",
      "module.FeatureExtraction.ConvNet.3.bias 64\n",
      "module.FeatureExtraction.ConvNet.6.weight 73728\n",
      "module.FeatureExtraction.ConvNet.6.bias 128\n",
      "module.FeatureExtraction.ConvNet.8.weight 147456\n",
      "module.FeatureExtraction.ConvNet.8.bias 128\n",
      "module.FeatureExtraction.ConvNet.11.weight 294912\n",
      "module.FeatureExtraction.ConvNet.12.weight 256\n",
      "module.FeatureExtraction.ConvNet.12.bias 256\n",
      "module.FeatureExtraction.ConvNet.14.weight 589824\n",
      "module.FeatureExtraction.ConvNet.15.weight 256\n",
      "module.FeatureExtraction.ConvNet.15.bias 256\n",
      "module.FeatureExtraction.ConvNet.18.weight 262144\n",
      "module.FeatureExtraction.ConvNet.18.bias 256\n",
      "module.SequenceModeling.0.rnn.weight_ih_l0 262144\n",
      "module.SequenceModeling.0.rnn.weight_hh_l0 262144\n",
      "module.SequenceModeling.0.rnn.bias_ih_l0 1024\n",
      "module.SequenceModeling.0.rnn.bias_hh_l0 1024\n",
      "module.SequenceModeling.0.rnn.weight_ih_l0_reverse 262144\n",
      "module.SequenceModeling.0.rnn.weight_hh_l0_reverse 262144\n",
      "module.SequenceModeling.0.rnn.bias_ih_l0_reverse 1024\n",
      "module.SequenceModeling.0.rnn.bias_hh_l0_reverse 1024\n",
      "module.SequenceModeling.0.linear.weight 131072\n",
      "module.SequenceModeling.0.linear.bias 256\n",
      "module.SequenceModeling.1.rnn.weight_ih_l0 262144\n",
      "module.SequenceModeling.1.rnn.weight_hh_l0 262144\n",
      "module.SequenceModeling.1.rnn.bias_ih_l0 1024\n",
      "module.SequenceModeling.1.rnn.bias_hh_l0 1024\n",
      "module.SequenceModeling.1.rnn.weight_ih_l0_reverse 262144\n",
      "module.SequenceModeling.1.rnn.weight_hh_l0_reverse 262144\n",
      "module.SequenceModeling.1.rnn.bias_ih_l0_reverse 1024\n",
      "module.SequenceModeling.1.rnn.bias_hh_l0_reverse 1024\n",
      "module.SequenceModeling.1.linear.weight 131072\n",
      "module.SequenceModeling.1.linear.bias 256\n",
      "module.Prediction.weight 23808\n",
      "module.Prediction.bias 93\n",
      "Total Trainable Params: 3780317\n",
      "Trainable params num :  3780317\n",
      "Optimizer:\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    decoupled_weight_decay: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "------------ Options -------------\n",
      "number: 0123456789\n",
      "symbol: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ ‚Ç¨\n",
      "lang_char: ‡∏Å‡∏Ç‡∏Ñ‡∏á‡∏à‡∏ä‡∏ã‡∏ç‡∏ì‡∏î‡∏ï‡∏ñ‡∏ó‡∏ò‡∏ô‡∏ö‡∏õ‡∏ú‡∏ù‡∏û‡∏ü‡∏°‡∏¢‡∏£‡∏•‡∏ß‡∏®‡∏™‡∏´‡∏≠‡∏∞‡∏±‡∏≤‡∏≥‡∏¥‡∏µ‡∏∂‡∏∑‡∏∏‡∏π‡πÄ‡πÅ‡πÇ‡πÉ‡πÑ‡πá‡πà‡πâ\n",
      "experiment_name: thai_auto\n",
      "train_data: all_data\n",
      "valid_data: all_data/thai_val\n",
      "manualSeed: 1111\n",
      "workers: 0\n",
      "batch_size: 8\n",
      "num_iter: 5000\n",
      "valInterval: 250\n",
      "saved_model: \n",
      "FT: False\n",
      "optim: adam\n",
      "lr: 0.001\n",
      "beta1: 0.9\n",
      "rho: 0.95\n",
      "eps: 1e-08\n",
      "grad_clip: 5\n",
      "select_data: ['thai_train']\n",
      "batch_ratio: ['1']\n",
      "total_data_usage_ratio: 1.0\n",
      "batch_max_length: 30\n",
      "imgH: 64\n",
      "imgW: 400\n",
      "rgb: False\n",
      "contrast_adjust: 0.0\n",
      "sensitive: True\n",
      "PAD: True\n",
      "data_filtering_off: False\n",
      "Transformation: None\n",
      "FeatureExtraction: VGG\n",
      "SequenceModeling: BiLSTM\n",
      "Prediction: CTC\n",
      "num_fiducial: 20\n",
      "input_channel: 1\n",
      "output_channel: 256\n",
      "hidden_size: 256\n",
      "decode: greedy\n",
      "new_prediction: False\n",
      "freeze_FeatureFxtraction: False\n",
      "freeze_SequenceModeling: False\n",
      "character: 0123456789!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ ‚Ç¨‡∏Å‡∏Ç‡∏Ñ‡∏á‡∏à‡∏ä‡∏ã‡∏ç‡∏ì‡∏î‡∏ï‡∏ñ‡∏ó‡∏ò‡∏ô‡∏ö‡∏õ‡∏ú‡∏ù‡∏û‡∏ü‡∏°‡∏¢‡∏£‡∏•‡∏ß‡∏®‡∏™‡∏´‡∏≠‡∏∞‡∏±‡∏≤‡∏≥‡∏¥‡∏µ‡∏∂‡∏∑‡∏∏‡∏π‡πÄ‡πÅ‡πÇ‡πÉ‡πÑ‡πá‡πà‡πâ\n",
      "num_class: 93\n",
      "---------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/puem/Desktop/tmp/EasyOCR/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/puem/Desktop/tmp/EasyOCR/trainer/train.py:173: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "/Users/puem/Desktop/tmp/EasyOCR/.venv/lib/python3.10/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/puem/Desktop/tmp/EasyOCR/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time:  68.41202092170715\n",
      "[250/5000] Train loss: 3.72254, Valid loss: 4.15305, Elapsed_time: 68.41333\n",
      "Current_accuracy : 0.000, Current_norm_ED  : 0.0658\n",
      "Best_accuracy    : 0.000, Best_norm_ED     : 0.0658\n",
      "--------------------------------------------------------------------------------\n",
      "Ground Truth              | Prediction                | Confidence Score & T/F\n",
      "--------------------------------------------------------------------------------\n",
      "‡∏™‡∏≥‡∏£‡∏ß‡∏à‡∏™‡∏¥‡πà‡∏á‡πÉ‡∏´‡∏°‡πà             | ‡∏ù‡∏±‡∏ô‡∏¢                      | 0.0000\tFalse\n",
      "‡∏ù‡∏±‡∏ô‡πÉ‡∏´‡πâ‡πÑ‡∏Å‡∏•                 | ‡∏ù‡∏±‡∏≤‡∏¢                      | 0.0006\tFalse\n",
      "‡πÑ‡∏õ‡πÉ‡∏´‡πâ‡∏ñ‡∏∂‡∏á                  | ‡∏ù‡∏±‡∏•                       | 0.0027\tFalse\n",
      "--------------------------------------------------------------------------------\n",
      "validation time:  0.30673885345458984\n",
      "training time:  69.56587886810303\n",
      "[500/5000] Train loss: 2.45274, Valid loss: 5.04326, Elapsed_time: 138.28604\n",
      "Current_accuracy : 0.000, Current_norm_ED  : 0.1094\n",
      "Best_accuracy    : 0.000, Best_norm_ED     : 0.1094\n",
      "--------------------------------------------------------------------------------\n",
      "Ground Truth              | Prediction                | Confidence Score & T/F\n",
      "--------------------------------------------------------------------------------\n",
      "‡∏ù‡∏±‡∏ô‡πÉ‡∏´‡πâ‡πÑ‡∏Å‡∏•                 | ‡∏Ñ‡∏£‡∏≠‡πà‡∏≤‡∏ô                    | 0.0000\tFalse\n",
      "‡∏û‡∏±‡∏Å‡∏ú‡πà‡∏≠‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠         | ‡∏ù‡∏ô‡∏≤‡∏ô‡∏´‡∏≤                    | 0.0000\tFalse\n",
      "‡πÑ‡∏õ‡πÉ‡∏´‡πâ‡∏ñ‡∏∂‡∏á                  | ‡∏£‡∏±‡∏≠‡∏ö‡πà‡∏≤‡∏¢                   | 0.0000\tFalse\n",
      "--------------------------------------------------------------------------------\n",
      "validation time:  0.28134870529174805\n",
      "training time:  69.41995906829834\n",
      "[750/5000] Train loss: 1.25727, Valid loss: 6.07624, Elapsed_time: 207.98746\n",
      "Current_accuracy : 0.000, Current_norm_ED  : 0.1073\n",
      "Best_accuracy    : 0.000, Best_norm_ED     : 0.1094\n",
      "--------------------------------------------------------------------------------\n",
      "Ground Truth              | Prediction                | Confidence Score & T/F\n",
      "--------------------------------------------------------------------------------\n",
      "‡πÇ‡∏•‡∏Å‡∏ô‡∏µ‡πâ‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏ç‡πà‡∏™‡πâ‡∏°‡∏ï‡∏≥‡∏£‡∏™‡πÅ‡∏ã‡πà | ‡∏Ñ‡∏ß‡∏Å‡∏ï‡πà‡∏≠‡∏¢                   | 0.0000\tFalse\n",
      "‡∏ó‡∏∞‡πÄ‡∏•‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°                | ‡∏ä‡∏µ‡∏≠‡∏ï‡∏Ñ‡∏£‡∏≤‡∏¢                  | 0.0000\tFalse\n",
      "‡∏™‡∏≥‡∏£‡∏ß‡∏à‡∏™‡∏¥‡πà‡∏á‡πÉ‡∏´‡∏°‡πà             | ‡πÄ‡∏£‡∏∞‡πÄ‡∏£‡∏´‡∏ç‡∏õ                  | 0.0003\tFalse\n",
      "--------------------------------------------------------------------------------\n",
      "validation time:  0.2665989398956299\n",
      "training time:  69.7788610458374\n",
      "[1000/5000] Train loss: 0.51330, Valid loss: 7.43578, Elapsed_time: 278.03315\n",
      "Current_accuracy : 0.000, Current_norm_ED  : 0.1330\n",
      "Best_accuracy    : 0.000, Best_norm_ED     : 0.1330\n",
      "--------------------------------------------------------------------------------\n",
      "Ground Truth              | Prediction                | Confidence Score & T/F\n",
      "--------------------------------------------------------------------------------\n",
      "‡∏û‡∏±‡∏Å‡∏ú‡πà‡∏≠‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠         | ‡∏£‡∏±‡∏Å‡πÄ‡∏´‡∏≤‡∏°                   | 0.0250\tFalse\n",
      "‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢                 | ‡∏Ñ‡∏î‡∏≠‡∏¢‡πÄ‡∏¢‡∏≠‡∏≤‡∏¢                 | 0.0007\tFalse\n",
      "‡∏ù‡∏ô‡∏ï‡∏Å‡πÄ‡∏¢‡πá‡∏ô‡∏™‡∏ö‡∏≤‡∏¢              | ‡∏ó‡∏∞‡∏á‡πÅ‡∏ã‡∏≠‡πà‡∏ö                  | 0.0158\tFalse\n",
      "--------------------------------------------------------------------------------\n",
      "validation time:  0.28064990043640137\n",
      "training time:  69.6744601726532\n",
      "[1250/5000] Train loss: 0.23410, Valid loss: 8.05225, Elapsed_time: 347.98842\n",
      "Current_accuracy : 0.000, Current_norm_ED  : 0.1117\n",
      "Best_accuracy    : 0.000, Best_norm_ED     : 0.1330\n",
      "--------------------------------------------------------------------------------\n",
      "Ground Truth              | Prediction                | Confidence Score & T/F\n",
      "--------------------------------------------------------------------------------\n",
      "‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ       | ‡∏£‡∏±‡∏Å‡∏≤‡∏≠‡∏ä‡πâ‡∏ô                  | 0.0000\tFalse\n",
      "‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á‡πÑ‡∏Å‡∏•                | ‡∏ù‡∏≤‡∏Å‡∏´‡∏≠‡∏≠‡∏∏                   | 0.0031\tFalse\n",
      "‡∏ï‡πâ‡∏°‡∏¢‡∏≥‡∏Å‡∏∏‡πâ‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´            | ‡πÇ‡∏≤‡∏Å‡∏ô‡∏≤‡∏Å                    | 0.0837\tFalse\n",
      "--------------------------------------------------------------------------------\n",
      "validation time:  0.2963719367980957\n",
      "training time:  69.82140707969666\n",
      "[1500/5000] Train loss: 0.24812, Valid loss: 9.69869, Elapsed_time: 418.10630\n",
      "Current_accuracy : 0.000, Current_norm_ED  : 0.1110\n",
      "Best_accuracy    : 0.000, Best_norm_ED     : 0.1330\n",
      "--------------------------------------------------------------------------------\n",
      "Ground Truth              | Prediction                | Confidence Score & T/F\n",
      "--------------------------------------------------------------------------------\n",
      "‡∏£‡∏≠‡∏¢‡∏¢‡∏¥‡πâ‡∏°‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∏‡∏Ç         | ‡πÑ‡∏õ‡πÉ‡∏´‡∏´‡∏´‡∏´‡∏´‡πâ‡∏°‡∏∂‡∏á              | 0.0000\tFalse\n",
      "‡∏™‡∏π‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï             | ‡∏™‡∏î‡∏¥‡∏ô‡∏û‡∏ô‡∏õ                   | 0.0006\tFalse\n",
      "‡πÑ‡∏õ‡πÉ‡∏´‡πâ‡∏ñ‡∏∂‡∏á                  | ‡∏õ‡∏±‡πÉ‡∏Å‡∏•                     | 0.0334\tFalse\n",
      "--------------------------------------------------------------------------------\n",
      "validation time:  0.2757148742675781\n",
      "training time:  70.53371977806091\n",
      "[1750/5000] Train loss: 0.09456, Valid loss: 10.41161, Elapsed_time: 488.91610\n",
      "Current_accuracy : 0.000, Current_norm_ED  : 0.1130\n",
      "Best_accuracy    : 0.000, Best_norm_ED     : 0.1330\n",
      "--------------------------------------------------------------------------------\n",
      "Ground Truth              | Prediction                | Confidence Score & T/F\n",
      "--------------------------------------------------------------------------------\n",
      "‡∏ó‡πâ‡∏≠‡∏á‡∏ü‡πâ‡∏≤‡∏™‡∏µ‡∏Ñ‡∏£‡∏≤‡∏°‡∏ó‡∏∞‡πÄ‡∏•‡∏™‡∏ß‡∏¢‡∏á‡∏≤    | ‡∏û‡∏∞‡πÄ‡∏ï‡πà‡∏ö‡∏´‡∏≤‡∏Å‡∏∂‡∏¢               | 0.0001\tFalse\n",
      "‡∏™‡∏≥‡∏£‡∏ß‡∏à‡∏™‡∏¥‡πà‡∏á‡πÉ‡∏´‡∏°‡πà             | ‡∏ß‡∏±‡∏ô‡∏õ‡∏≤‡∏°‡∏ô‡∏û‡∏≤‡∏¢                | 0.0015\tFalse\n",
      "‡∏ï‡πâ‡∏°‡∏¢‡∏≥‡∏Å‡∏∏‡πâ‡∏á                 | ‡∏ù‡∏ô‡∏ï‡πâ‡∏±‡πÄ‡∏≠‡∏¢                  | 0.0076\tFalse\n",
      "--------------------------------------------------------------------------------\n",
      "validation time:  0.27532196044921875\n",
      "training time:  69.79638981819153\n",
      "[2000/5000] Train loss: 0.01967, Valid loss: 10.71595, Elapsed_time: 558.98794\n",
      "Current_accuracy : 0.000, Current_norm_ED  : 0.1089\n",
      "Best_accuracy    : 0.000, Best_norm_ED     : 0.1330\n",
      "--------------------------------------------------------------------------------\n",
      "Ground Truth              | Prediction                | Confidence Score & T/F\n",
      "--------------------------------------------------------------------------------\n",
      "‡∏™‡∏≥‡∏£‡∏ß‡∏à‡∏™‡∏¥‡πà‡∏á‡πÉ‡∏´‡∏°‡πà             | ‡πÄ‡∏î‡∏¥‡∏ô‡∏µ‡πâ‡∏≤‡πâ‡πÄ‡∏ç‡πà               | 0.0004\tFalse\n",
      "‡∏û‡∏±‡∏Å‡∏ú‡πà‡∏≠‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠         | ‡∏£‡∏±‡∏Å‡πÄ‡∏´‡∏≤‡∏°                   | 0.0895\tFalse\n",
      "‡∏™‡∏π‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï             | ‡∏ß‡∏±‡∏¥‡πâ‡∏≠‡πÄ‡∏≠‡πÄ‡∏ç‡∏õ                | 0.0000\tFalse\n",
      "--------------------------------------------------------------------------------\n",
      "validation time:  0.29607605934143066\n",
      "training time:  70.52191996574402\n",
      "[2250/5000] Train loss: 0.19972, Valid loss: 10.98818, Elapsed_time: 629.80614\n",
      "Current_accuracy : 0.000, Current_norm_ED  : 0.1106\n",
      "Best_accuracy    : 0.000, Best_norm_ED     : 0.1330\n",
      "--------------------------------------------------------------------------------\n",
      "Ground Truth              | Prediction                | Confidence Score & T/F\n",
      "--------------------------------------------------------------------------------\n",
      "‡∏ï‡πâ‡∏°‡∏¢‡∏≥‡∏Å‡∏∏‡πâ‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´            | ‡∏£‡∏±‡∏Å‡∏≤‡∏ß‡∏≠‡∏≤                   | 0.0202\tFalse\n",
      "‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢                 | ‡∏ù‡∏î‡∏ï‡∏¢‡πÄ‡∏¢‡∏ó‡∏¢                  | 0.0204\tFalse\n",
      "‡∏™‡∏π‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï             | ‡πÇ‡∏≠‡∏¥‡∏ô‡∏ó‡∏≠‡πà‡∏£‡∏≠‡∏≠‡∏≠‡πÄ‡∏ô‡∏£‡πà‡∏Å‡∏õ         | 0.0000\tFalse\n",
      "--------------------------------------------------------------------------------\n",
      "validation time:  0.2603321075439453\n",
      "training time:  70.38989615440369\n",
      "[2500/5000] Train loss: 0.02061, Valid loss: 12.06124, Elapsed_time: 700.45645\n",
      "Current_accuracy : 0.000, Current_norm_ED  : 0.1143\n",
      "Best_accuracy    : 0.000, Best_norm_ED     : 0.1330\n",
      "--------------------------------------------------------------------------------\n",
      "Ground Truth              | Prediction                | Confidence Score & T/F\n",
      "--------------------------------------------------------------------------------\n",
      "‡∏™‡∏≥‡∏£‡∏ß‡∏à‡∏™‡∏¥‡πà‡∏á‡πÉ‡∏´‡∏°‡πà             | ‡πÄ‡∏±‡∏ô‡∏õ‡πà‡∏≤‡∏°‡∏ç‡∏≤‡∏•                | 0.0301\tFalse\n",
      "‡∏ï‡πâ‡∏°‡∏¢‡∏≥‡∏Å‡∏∏‡πâ‡∏á                 | ‡∏ù‡∏ô‡∏ï‡∏¢‡∏ï‡∏Ñ‡∏£‡∏±‡πá‡∏ö‡∏≠‡∏á              | 0.0103\tFalse\n",
      "‡∏û‡∏±‡∏Å‡∏ú‡πà‡∏≠‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠         | ‡∏£‡∏±‡∏Å‡∏Å‡∏á‡πÄ‡∏´‡∏≤‡πà                 | 0.0253\tFalse\n",
      "--------------------------------------------------------------------------------\n",
      "validation time:  0.26273298263549805\n",
      "training time:  70.84592580795288\n",
      "[2750/5000] Train loss: 0.00375, Valid loss: 12.29717, Elapsed_time: 771.56523\n",
      "Current_accuracy : 0.000, Current_norm_ED  : 0.1192\n",
      "Best_accuracy    : 0.000, Best_norm_ED     : 0.1330\n",
      "--------------------------------------------------------------------------------\n",
      "Ground Truth              | Prediction                | Confidence Score & T/F\n",
      "--------------------------------------------------------------------------------\n",
      "‡∏ï‡πâ‡∏°‡∏¢‡∏≥‡∏Å‡∏∏‡πâ‡∏á                 | ‡∏ù‡∏ô‡∏ï‡∏¢‡πÄ‡∏Ñ‡∏£‡∏±‡∏ö‡∏≠‡∏¢               | 0.0262\tFalse\n",
      "‡∏ó‡∏∞‡πÄ‡∏•‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°                | ‡∏ù‡∏£‡∏≠‡∏ö‡∏Ñ‡∏£‡∏°‡∏£‡∏≤‡∏¢                | 0.0144\tFalse\n",
      "‡∏™‡∏≥‡∏£‡∏ß‡∏à‡∏™‡∏¥‡πà‡∏á‡πÉ‡∏´‡∏°‡πà             | ‡∏ï‡πâ‡∏≠‡∏¢‡∏≥‡πÄ‡∏≠‡∏á                  | 0.2574\tFalse\n",
      "--------------------------------------------------------------------------------\n",
      "validation time:  0.2701597213745117\n",
      "training time:  71.28454494476318\n",
      "[3000/5000] Train loss: 0.00175, Valid loss: 12.50004, Elapsed_time: 843.12016\n",
      "Current_accuracy : 0.000, Current_norm_ED  : 0.1131\n",
      "Best_accuracy    : 0.000, Best_norm_ED     : 0.1330\n",
      "--------------------------------------------------------------------------------\n",
      "Ground Truth              | Prediction                | Confidence Score & T/F\n",
      "--------------------------------------------------------------------------------\n",
      "‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ       | ‡∏£‡∏±‡∏Å‡∏¢‡∏ô‡∏ö‡∏±‡∏±‡∏ß‡∏≠‡∏ä‡πâ‡∏≤             | 0.0114\tFalse\n",
      "‡∏™‡∏≥‡∏£‡∏ß‡∏à‡∏™‡∏¥‡πà‡∏á‡πÉ‡∏´‡∏°‡πà             | ‡∏ß‡∏±‡∏ô‡∏õ‡πà‡∏≤‡∏°‡∏ç‡∏≤‡∏•                | 0.0258\tFalse\n",
      "‡πÇ‡∏•‡∏Å‡∏ô‡∏µ‡πâ‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏ç‡πà‡∏™‡πâ‡∏°‡∏ï‡∏≥‡∏£‡∏™‡πÅ‡∏ã‡πà | ‡∏£‡∏±‡∏Å‡∏ú‡πà‡∏≠‡πà‡∏≠‡∏£‡∏±‡∏£‡πÄ‡πÄ‡∏¢‡∏´‡∏≠‡∏¢         | 0.0000\tFalse\n",
      "--------------------------------------------------------------------------------\n",
      "validation time:  0.2581939697265625\n",
      "training time:  71.33869695663452\n",
      "[3250/5000] Train loss: 0.00125, Valid loss: 12.64638, Elapsed_time: 914.71713\n",
      "Current_accuracy : 0.000, Current_norm_ED  : 0.1156\n",
      "Best_accuracy    : 0.000, Best_norm_ED     : 0.1330\n",
      "--------------------------------------------------------------------------------\n",
      "Ground Truth              | Prediction                | Confidence Score & T/F\n",
      "--------------------------------------------------------------------------------\n",
      "‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢                 | ‡∏ù‡πà‡∏ï‡∏¢‡πÄ‡∏¢‡πá‡∏°‡∏ö‡πà‡∏¢               | 0.0113\tFalse\n",
      "‡∏£‡∏≠‡∏¢‡∏¢‡∏¥‡πâ‡∏°‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∏‡∏Ç         | ‡πÑ‡∏õ‡πÉ‡∏±‡∏´‡πâ‡∏°‡πà                  | 0.0295\tFalse\n",
      "‡πÇ‡∏•‡∏Å‡∏ô‡∏µ‡πâ‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏ç‡πà‡∏™‡πâ‡∏°‡∏ï‡∏≥‡∏£‡∏™‡πÅ‡∏ã‡πà | ‡∏û‡∏±‡∏Å‡∏ú‡πà‡πà‡∏≠‡∏≠‡∏£‡∏±‡∏£‡πÄ‡πÄ‡∏¢‡πà‡∏≠‡∏¢         | 0.0000\tFalse\n",
      "--------------------------------------------------------------------------------\n",
      "validation time:  0.2628347873687744\n",
      "training time:  72.53227591514587\n",
      "[3500/5000] Train loss: 0.44180, Valid loss: 9.99606, Elapsed_time: 987.51273\n",
      "Current_accuracy : 0.000, Current_norm_ED  : 0.0903\n",
      "Best_accuracy    : 0.000, Best_norm_ED     : 0.1330\n",
      "--------------------------------------------------------------------------------\n",
      "Ground Truth              | Prediction                | Confidence Score & T/F\n",
      "--------------------------------------------------------------------------------\n",
      "‡∏ù‡∏ô‡∏ï‡∏Å‡πÄ‡∏¢‡πá‡∏ô‡∏™‡∏ö‡∏≤‡∏¢              | ‡∏£‡∏±‡∏Å‡∏ï‡∏ô‡πÅ‡∏ã‡∏ô‡πà‡∏ô                | 0.0025\tFalse\n",
      "‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢                 | ‡∏Ñ‡∏£‡∏ß‡∏ï‡∏Ñ‡∏¢‡πâ‡∏≤‡∏¢                 | 0.0040\tFalse\n",
      "‡πÑ‡∏õ‡πÉ‡∏´‡πâ‡∏ñ‡∏∂‡∏á                  | ‡∏ù‡∏£‡πÉ‡πâ‡πÑ‡∏Å‡∏•                   | 0.0115\tFalse\n",
      "--------------------------------------------------------------------------------\n",
      "validation time:  0.32875967025756836\n",
      "training time:  74.61462783813477\n",
      "[3750/5000] Train loss: 0.10380, Valid loss: 11.14993, Elapsed_time: 1062.45648\n",
      "Current_accuracy : 0.000, Current_norm_ED  : 0.0958\n",
      "Best_accuracy    : 0.000, Best_norm_ED     : 0.1330\n",
      "--------------------------------------------------------------------------------\n",
      "Ground Truth              | Prediction                | Confidence Score & T/F\n",
      "--------------------------------------------------------------------------------\n",
      "‡∏™‡∏≥‡∏£‡∏ß‡∏à‡∏™‡∏¥‡πà‡∏á‡πÉ‡∏´‡∏°‡πà             | ‡πÄ‡∏î‡∏¥‡∏¢‡∏ó‡∏®‡∏≤‡∏ó‡∏µ‡∏¢                | 0.0197\tFalse\n",
      "‡∏™‡∏π‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï             | ‡∏ä‡∏¥‡∏ó‡∏µ‡πâ‡∏≠‡∏≠‡∏≠‡∏£‡πÄ‡∏£‡∏ô‡∏£‡πà‡∏π‡πâ          | 0.0000\tFalse\n",
      "‡∏™‡∏≥‡∏£‡∏ß‡∏à‡∏™‡∏¥‡πà‡∏á‡πÉ‡∏´‡∏°‡πà             | ‡∏Ñ‡πâ‡∏∞‡∏ö‡∏ó‡∏≥‡∏ó‡∏∂‡∏•                 | 0.0344\tFalse\n",
      "--------------------------------------------------------------------------------\n",
      "validation time:  0.2636868953704834\n",
      "training time:  70.77816891670227\n",
      "[4000/5000] Train loss: 0.00941, Valid loss: 11.94557, Elapsed_time: 1133.49845\n",
      "Current_accuracy : 0.000, Current_norm_ED  : 0.1026\n",
      "Best_accuracy    : 0.000, Best_norm_ED     : 0.1330\n",
      "--------------------------------------------------------------------------------\n",
      "Ground Truth              | Prediction                | Confidence Score & T/F\n",
      "--------------------------------------------------------------------------------\n",
      "‡∏û‡∏±‡∏Å‡∏ú‡πà‡∏≠‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠         | ‡πÄ‡∏ß‡∏Å‡∏ô‡∏ó‡∏µ‡∏≤‡πâ‡∏á‡∏ô‡∏ß‡∏±‡∏ß‡∏±‡∏ô‡∏ß‡∏û‡∏≤‡∏¢       | 0.0000\tFalse\n",
      "‡∏™‡∏≥‡∏£‡∏ß‡∏à‡∏™‡∏¥‡πà‡∏á‡πÉ‡∏´‡∏°‡πà             | ‡∏ß‡∏≤‡∏ô‡∏õ‡πà‡∏≤‡∏°‡∏£‡∏û‡∏≤‡∏π‡∏¢              | 0.0004\tFalse\n",
      "‡πÇ‡∏•‡∏Å‡∏ô‡∏µ‡πâ‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏ç‡πà‡∏™‡πâ‡∏°‡∏ï‡∏≥‡∏£‡∏™‡πÅ‡∏ã‡πà | ‡∏û‡∏±‡∏Å‡∏ï‡∏Å‡πÄ‡∏£‡πÄ‡πÄ‡∏´‡∏∏‡∏≠‡∏¢             | 0.0000\tFalse\n",
      "--------------------------------------------------------------------------------\n",
      "validation time:  0.2690460681915283\n",
      "training time:  71.51432514190674\n",
      "[4250/5000] Train loss: 0.01209, Valid loss: 12.26807, Elapsed_time: 1205.28193\n",
      "Current_accuracy : 0.000, Current_norm_ED  : 0.0972\n",
      "Best_accuracy    : 0.000, Best_norm_ED     : 0.1330\n",
      "--------------------------------------------------------------------------------\n",
      "Ground Truth              | Prediction                | Confidence Score & T/F\n",
      "--------------------------------------------------------------------------------\n",
      "‡πÑ‡∏õ‡πÉ‡∏´‡πâ‡∏ñ‡∏∂‡∏á                  | ‡∏ù‡∏±‡πÉ‡∏´‡πâ‡∏Å‡∏•                   | 0.4054\tFalse\n",
      "‡∏ï‡πâ‡∏°‡∏¢‡∏≥‡∏Å‡∏∏‡πâ‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´            | ‡∏£‡∏±‡∏Å‡∏≠‡∏Å                     | 0.4635\tFalse\n",
      "‡∏ï‡πâ‡∏°‡∏¢‡∏≥‡∏Å‡∏∏‡πâ‡∏á                 | ‡∏ù‡∏ô‡∏ö‡πÄ‡∏£‡∏±‡∏≠‡∏á                  | 0.0070\tFalse\n",
      "--------------------------------------------------------------------------------\n",
      "validation time:  0.26219701766967773\n",
      "training time:  71.51347827911377\n",
      "[4500/5000] Train loss: 0.79203, Valid loss: 12.20051, Elapsed_time: 1277.05833\n",
      "Current_accuracy : 0.000, Current_norm_ED  : 0.0770\n",
      "Best_accuracy    : 0.000, Best_norm_ED     : 0.1330\n",
      "--------------------------------------------------------------------------------\n",
      "Ground Truth              | Prediction                | Confidence Score & T/F\n",
      "--------------------------------------------------------------------------------\n",
      "‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ       | ‡πÄ‡∏Ç‡∏Å‡∏¢‡∏Å‡∏ö‡∏±‡∏ß‡∏∑‡∏ì‡∏ä‡πâ‡∏≤             | 0.0001\tFalse\n",
      "‡∏£‡∏±‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á                 | ‡∏ù‡∏±‡πÅ‡πÄ‡∏¢‡πÑ‡∏°‡∏ö‡∏≤‡∏¢                | 0.0251\tFalse\n",
      "‡πÇ‡∏•‡∏Å‡∏ô‡∏µ‡πâ‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏ç‡πà‡∏™‡πâ‡∏°‡∏ï‡∏≥‡∏£‡∏™‡πÅ‡∏ã‡πà | ‡∏£‡∏±‡∏Å‡∏ï‡∏£‡∏±‡∏¢‡∏¢‡∏Å‡∏≠‡∏¢               | 0.0000\tFalse\n",
      "--------------------------------------------------------------------------------\n",
      "validation time:  0.24909400939941406\n",
      "training time:  71.29307222366333\n",
      "[4750/5000] Train loss: 0.01869, Valid loss: 12.76912, Elapsed_time: 1348.60058\n",
      "Current_accuracy : 0.000, Current_norm_ED  : 0.0973\n",
      "Best_accuracy    : 0.000, Best_norm_ED     : 0.1330\n",
      "--------------------------------------------------------------------------------\n",
      "Ground Truth              | Prediction                | Confidence Score & T/F\n",
      "--------------------------------------------------------------------------------\n",
      "‡∏™‡∏π‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï             | ‡πÇ‡∏≠‡∏≤‡∏°‡∏ó‡πâ‡∏≠‡πà‡∏≠‡∏ç‡πâ               | 0.0000\tFalse\n",
      "‡∏û‡∏±‡∏Å‡∏ú‡πà‡∏≠‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠         | ‡∏ù‡∏Ç‡∏ß‡∏°‡πÄ‡∏ç‡πà‡∏®‡πÑ‡∏ô‡∏ô‡∏ß‡∏ß‡∏ô‡∏ö‡∏≤‡∏¢         | 0.0001\tFalse\n",
      "‡∏£‡∏±‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á                 | ‡∏ù‡∏≠‡πÅ‡πÄ‡∏¢‡∏°‡∏ö‡∏≤‡∏¢                 | 0.1864\tFalse\n",
      "--------------------------------------------------------------------------------\n",
      "validation time:  0.266812801361084\n",
      "training time:  70.72238612174988\n",
      "[5000/5000] Train loss: 0.00613, Valid loss: 12.58106, Elapsed_time: 1419.58994\n",
      "Current_accuracy : 0.000, Current_norm_ED  : 0.1360\n",
      "Best_accuracy    : 0.000, Best_norm_ED     : 0.1360\n",
      "--------------------------------------------------------------------------------\n",
      "Ground Truth              | Prediction                | Confidence Score & T/F\n",
      "--------------------------------------------------------------------------------\n",
      "‡∏™‡∏π‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï             | ‡πÇ‡∏£‡∏¥‡∏ô‡∏ó‡πâ‡∏≠‡∏≠‡∏ç‡πà                | 0.0004\tFalse\n",
      "‡∏ï‡πâ‡∏°‡∏¢‡∏≥‡∏Å‡∏∏‡πâ‡∏á                 | ‡∏ù‡∏Ç‡∏ï‡∏ö‡∏Ñ‡∏¢‡∏±‡πá‡∏≠‡∏ö‡∏≠‡∏á              | 0.0089\tFalse\n",
      "‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á‡πÑ‡∏Å‡∏•                | ‡∏ó‡∏∞‡πÄ‡∏•‡∏™‡∏≠‡∏∏                   | 0.0968\tFalse\n",
      "--------------------------------------------------------------------------------\n",
      "validation time:  0.277972936630249\n",
      "end the training\n",
      "\n",
      "üèÅ Training session ended at: 2025-06-28 02:06:15\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/puem/Desktop/tmp/EasyOCR/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3587: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Start model training\n",
    "print(\"=\"*50)\n",
    "print(\"STARTING MODEL TRAINING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Training parameters\n",
    "use_amp = False  # Set to True to enable Automatic Mixed Precision for faster training\n",
    "show_samples = 3  # Number of prediction samples to show during validation\n",
    "\n",
    "print(f\"üöÄ Starting training at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üìä Training samples to show: {show_samples}\")\n",
    "print(f\"‚ö° Mixed precision (AMP): {'Enabled' if use_amp else 'Disabled'}\")\n",
    "\n",
    "# Check if resuming from checkpoint\n",
    "if opt.saved_model and opt.saved_model != '':\n",
    "    print(f\"üîÑ Resuming training from: {opt.saved_model}\")\n",
    "else:\n",
    "    print(\"üÜï Starting training from scratch\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING LOG\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    # Start training\n",
    "    train(opt, show_number=show_samples, amp=use_amp)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚ö†Ô∏è  Training interrupted by user (Ctrl+C)\")\n",
    "    print(\"Model checkpoints are saved in: ./saved_models/{}/\".format(opt.experiment_name))\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Training failed with error: {e}\")\n",
    "    print(\"Please check the error details above and ensure:\")\n",
    "    print(\"1. Data paths are correct\")\n",
    "    print(\"2. Required dependencies are installed\")\n",
    "    print(\"3. GPU memory is sufficient\")\n",
    "    raise\n",
    "finally:\n",
    "    print(f\"\\nüèÅ Training session ended at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ba44b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training monitoring utilities\n",
    "print(\"=\"*50)\n",
    "print(\"TRAINING MONITORING UTILITIES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def check_training_logs(experiment_name):\n",
    "    \"\"\"Check and display training logs\"\"\"\n",
    "    log_dir = f\"./saved_models/{experiment_name}\"\n",
    "    \n",
    "    if not os.path.exists(log_dir):\n",
    "        print(f\"‚ùå Log directory not found: {log_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üìÅ Log directory: {log_dir}\")\n",
    "    \n",
    "    # Check available log files\n",
    "    log_files = []\n",
    "    for file in os.listdir(log_dir):\n",
    "        if file.endswith('.txt') or file.endswith('.log'):\n",
    "            log_files.append(file)\n",
    "    \n",
    "    if log_files:\n",
    "        print(\"üìÑ Available log files:\")\n",
    "        for log_file in log_files:\n",
    "            file_path = os.path.join(log_dir, log_file)\n",
    "            file_size = os.path.getsize(file_path)\n",
    "            print(f\"   - {log_file} ({file_size} bytes)\")\n",
    "    else:\n",
    "        print(\"‚ùå No log files found\")\n",
    "    \n",
    "    # Check available model checkpoints\n",
    "    model_files = []\n",
    "    for file in os.listdir(log_dir):\n",
    "        if file.endswith('.pth'):\n",
    "            model_files.append(file)\n",
    "    \n",
    "    if model_files:\n",
    "        print(\"üíæ Available model checkpoints:\")\n",
    "        for model_file in model_files:\n",
    "            file_path = os.path.join(log_dir, model_file)\n",
    "            file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB\n",
    "            print(f\"   - {model_file} ({file_size:.1f} MB)\")\n",
    "    else:\n",
    "        print(\"‚ùå No model checkpoints found\")\n",
    "\n",
    "def read_training_log(experiment_name, lines=20):\n",
    "    \"\"\"Read the last N lines of training log\"\"\"\n",
    "    log_path = f\"./saved_models/{experiment_name}/log_train.txt\"\n",
    "    \n",
    "    if not os.path.exists(log_path):\n",
    "        print(f\"‚ùå Training log not found: {log_path}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üìÑ Last {lines} lines of training log:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    with open(log_path, 'r', encoding='utf8') as f:\n",
    "        log_lines = f.readlines()\n",
    "        for line in log_lines[-lines:]:\n",
    "            print(line.strip())\n",
    "\n",
    "# Example usage (uncomment after training starts):\n",
    "# check_training_logs(opt.experiment_name)\n",
    "# read_training_log(opt.experiment_name, lines=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadf79c5",
   "metadata": {},
   "source": [
    "## üìù Training Notes and Tips\n",
    "\n",
    "### Important Notes:\n",
    "1. **Memory Management**: Training requires significant GPU memory. Reduce batch size if you encounter out-of-memory errors.\n",
    "2. **Checkpointing**: Models are automatically saved every 10,000 iterations and when achieving best accuracy/norm_ED.\n",
    "3. **Monitoring**: Use the utility functions above to monitor training progress and check logs.\n",
    "4. **Interruption**: You can safely interrupt training with Ctrl+C - checkpoints are regularly saved.\n",
    "\n",
    "### Configuration Tips:\n",
    "- **Mixed Precision (AMP)**: Enable for faster training on modern GPUs, but may affect model quality\n",
    "- **Learning Rate**: Start with default values, adjust based on loss convergence\n",
    "- **Validation Interval**: More frequent validation gives better monitoring but slows training\n",
    "- **Batch Size**: Larger batches generally improve training stability\n",
    "\n",
    "### After Training:\n",
    "1. Check the `saved_models/{experiment_name}/` directory for:\n",
    "   - `best_accuracy.pth` - Model with highest accuracy\n",
    "   - `best_norm_ED.pth` - Model with best normalized edit distance\n",
    "   - `iter_*.pth` - Regular checkpoints\n",
    "   - `log_train.txt` - Training progress log\n",
    "   - `opt.txt` - Configuration used for training\n",
    "\n",
    "2. Use the monitoring utilities to analyze training progress\n",
    "3. Evaluate the trained model on your test dataset\n",
    "4. Consider fine-tuning with different learning rates or datasets\n",
    "\n",
    "### Troubleshooting:\n",
    "- **CUDA out of memory**: Reduce batch size or image dimensions\n",
    "- **Slow training**: Enable AMP, increase num_workers, or use smaller validation intervals\n",
    "- **Poor convergence**: Check learning rate, ensure data quality, verify character set\n",
    "- **File not found errors**: Verify data paths and configuration file locations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "easyocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
