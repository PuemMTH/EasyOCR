#!/usr/bin/env python3
"""
EasyOCR Thai Training Pipeline - Essential Steps Only
Pipeline ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• EasyOCR ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ (‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô)
"""

import os
import sys
import json
import shutil
import subprocess
from pathlib import Path
import csv

class EasyOCRPipeline:
    def __init__(self, data_dir="all_data/results_JS-Kobori", model_name="thai_model"):
        self.data_dir = Path(data_dir)
        self.model_name = model_name
        self.train_dir = Path("all_data/thai_train")
        self.config_dir = Path("config_files")
        self.saved_dir = Path(f"saved_models/{model_name}")
        
    def step1_check_data(self):
        """Step 1: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"""
        print("üìã Step 1: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        if not self.data_dir.exists():
            raise FileNotFoundError(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {self.data_dir}")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå gt.txt
        gt_file = self.data_dir / "gt.txt"
        if not gt_file.exists():
            raise FileNotFoundError(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå gt.txt: {gt_file}")
        
        # ‡∏ô‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        with open(gt_file, 'r', encoding='utf-8') as f:
            data_count = len(f.readlines())
        
        print(f"‚úÖ ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {data_count} ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á")
        return data_count
    
    def step2_prepare_data(self):
        """Step 2: ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"""
        print("üìä Step 2: ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå
        self.train_dir.mkdir(parents=True, exist_ok=True)
        val_dir = Path("all_data/thai_val")
        val_dir.mkdir(parents=True, exist_ok=True)
        
        # ‡∏≠‡πà‡∏≤‡∏ô gt.txt
        gt_file = self.data_dir / "gt.txt"
        labels_data = []
        
        with open(gt_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    parts = line.split('\t')
                    if len(parts) >= 2:
                        image_path = parts[0].replace('\\', '/')
                        text = parts[1]
                        filename = os.path.basename(image_path)
                        labels_data.append([filename, text])
        
        # ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• train/val (80:20)
        import random
        random.seed(42)  # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
        random.shuffle(labels_data)
        
        split_idx = int(len(labels_data) * 0.8)
        train_data = labels_data[:split_idx]
        val_data = labels_data[split_idx:]
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á labels.csv ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö train
        train_csv = self.train_dir / 'labels.csv'
        with open(train_csv, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(['filename', 'words'])
            writer.writerows(train_data)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á labels.csv ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö val
        val_csv = val_dir / 'labels.csv'
        with open(val_csv, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(['filename', 'words'])
            writer.writerows(val_data)
        
        # ‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
        source_images_dir = self.data_dir / "images" / "0"
        if source_images_dir.exists():
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á set ‡∏Ç‡∏≠‡∏á filenames ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö train ‡πÅ‡∏•‡∏∞ val
            train_files = {item[0] for item in train_data}
            val_files = {item[0] for item in val_data}
            
            train_copied = 0
            val_copied = 0
            
            for filename in os.listdir(source_images_dir):
                if filename.lower().endswith(('.jpg', '.jpeg', '.png')):
                    source_path = source_images_dir / filename
                    
                    if filename in train_files:
                        target_path = self.train_dir / filename
                        shutil.copy2(source_path, target_path)
                        train_copied += 1
                    elif filename in val_files:
                        target_path = val_dir / filename
                        shutil.copy2(source_path, target_path)
                        val_copied += 1
            
            print(f"‚úÖ ‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å‡∏£‡∏π‡∏õ Train: {train_copied} ‡πÑ‡∏ü‡∏•‡πå")
            print(f"‚úÖ ‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å‡∏£‡∏π‡∏õ Val: {val_copied} ‡πÑ‡∏ü‡∏•‡πå")
        
        print(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á Train labels: {len(train_data)} ‡πÅ‡∏ñ‡∏ß")
        print(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á Val labels: {len(val_data)} ‡πÅ‡∏ñ‡∏ß")
        return labels_data
    
    def step3_analyze_parameters(self, labels_data):
        """Step 3: ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå parameters (‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô)"""
        print("‚öôÔ∏è Step 3: ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå parameters")
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        text_lengths = [len(item[1]) for item in labels_data]
        max_length = max(text_lengths)
        recommended_max_length = min(max_length + 5, 50)  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 50
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£
        all_text = ''.join([item[1] for item in labels_data])
        unique_chars = sorted(set(all_text))
        
        print(f"‚úÖ ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£: {len(unique_chars)} ‡∏ï‡∏±‡∏ß")
        print(f"‚úÖ ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î: {max_length} -> ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥: {recommended_max_length}")
        
        return {
            'batch_max_length': recommended_max_length,
            'lang_char': ''.join(unique_chars),
            'num_chars': len(unique_chars)
        }
    
    def step4_create_config(self, params):
        """Step 4: ‡∏™‡∏£‡πâ‡∏≤‡∏á config file"""
        print("üìù Step 4: ‡∏™‡∏£‡πâ‡∏≤‡∏á config")
        
        self.config_dir.mkdir(exist_ok=True)
        
        config = f"""# Essential Thai OCR Config
number: '0123456789'
symbol: "!\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{{|}}~ ‚Ç¨"
lang_char: '{params['lang_char']}'
experiment_name: '{self.model_name}'
train_data: 'all_data'
valid_data: 'all_data'
manualSeed: 1111
workers: 0
batch_size: 8
num_iter: 5000
valInterval: 250
saved_model: ''
FT: False
optim: 'adam'
lr: 0.001
beta1: 0.9
rho: 0.95
eps: 0.00000001
grad_clip: 5
select_data: 'thai_train-thai_val'
batch_ratio: '0.8-0.2'
total_data_usage_ratio: 1.0
batch_max_length: {params['batch_max_length']}
imgH: 64
imgW: 400
rgb: False
contrast_adjust: 0.0
sensitive: True
PAD: True
data_filtering_off: False
Transformation: 'None'
FeatureExtraction: 'VGG'
SequenceModeling: 'BiLSTM'
Prediction: 'CTC'
num_fiducial: 20
input_channel: 1
output_channel: 256
hidden_size: 256
decode: 'greedy'
new_prediction: False
freeze_FeatureFxtraction: False
freeze_SequenceModeling: False"""
        
        config_file = self.config_dir / f"{self.model_name}_config.yaml"
        with open(config_file, 'w', encoding='utf-8') as f:
            f.write(config)
        
        print(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á config: {config_file}")
        return config_file
    
    def step5_install_dependencies(self):
        """Step 5: ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies (‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô)"""
        print("üì¶ Step 5: ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies")
        
        essential_packages = [
            "torch", "torchvision", "torchaudio",
            "opencv-python", "pillow", "numpy",
            "pyyaml", "pandas", "easydict"
        ]
        
        try:
            cmd = ["uv", "add"] + essential_packages
            subprocess.run(cmd, check=True, capture_output=True)
            print(f"‚úÖ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡πâ‡∏ß: {', '.join(essential_packages)}")
        except subprocess.CalledProcessError as e:
            print(f"‚ùå ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {e}")
            return False
        
        return True
    
    def step6_start_training(self, config_file):
        """Step 6: ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å"""
        print("üöÄ Step 6: ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á training script ‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢
        train_script = f"""
import os
import torch.backends.cudnn as cudnn
import yaml
from train import train
from utils import AttrDict

cudnn.benchmark = True
cudnn.deterministic = False

def get_config(file_path):
    with open(file_path, 'r', encoding="utf8") as stream:
        opt = yaml.safe_load(stream)
    opt = AttrDict(opt)
    opt.character = opt.number + opt.symbol + opt.lang_char
    os.makedirs(f'./saved_models/{{opt.experiment_name}}', exist_ok=True)
    return opt

if __name__ == '__main__':
    print("Loading config from: {config_file}")
    opt = get_config("{config_file}")
    print(f"Starting training: {{opt.experiment_name}}")
    print(f"Character set size: {{len(opt.character)}}")
    
    try:
        train(opt, amp=False)
    except Exception as e:
        print(f"Training error: {{e}}")
        print("Check log files in saved_models/ directory")
"""
        
        train_file = Path("quick_train.py")
        with open(train_file, 'w', encoding='utf-8') as f:
            f.write(train_script)
        
        print(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á training script: {train_file}")
        print("üí° ‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏î‡πâ‡∏ß‡∏¢: uv run python quick_train.py")
        
        return train_file
    
    def run_pipeline(self):
        """‡∏£‡∏±‡∏ô pipeline ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"""
        print("üîÑ EasyOCR Thai Training Pipeline")
        print("=" * 50)
        
        try:
            # Step 1: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            data_count = self.step1_check_data()
            
            # Step 2: ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            labels_data = self.step2_prepare_data()
            
            # Step 3: ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå parameters
            params = self.step3_analyze_parameters(labels_data)
            
            # Step 4: ‡∏™‡∏£‡πâ‡∏≤‡∏á config
            config_file = self.step4_create_config(params)
            
            # Step 5: ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies
            if not self.step5_install_dependencies():
                print("‚ùå ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß")
                return False
            
            # Step 6: ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å
            train_file = self.step6_start_training(config_file)
            
            print("\n" + "=" * 50)
            print("‚úÖ Pipeline ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!")
            print(f"üìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {data_count} ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á")
            print(f"üî§ ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£: {params['num_chars']} ‡∏ï‡∏±‡∏ß")
            print(f"üìè ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î: {params['batch_max_length']}")
            print(f"‚öôÔ∏è Config: {config_file}")
            print(f"üöÄ Training script: {train_file}")
            
            print("\nüéØ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ:")
            print("1. ‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å: uv run python quick_train.py")
            print("2. ‡∏î‡∏π log ‡πÉ‡∏ô: saved_models/thai_model/")
            print("3. ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÉ‡∏ô: saved_models/thai_model/best_accuracy.pth")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Pipeline ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {e}")
            return False

def main():
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å"""
    import argparse
    
    parser = argparse.ArgumentParser(description="EasyOCR Thai Training Pipeline")
    parser.add_argument("--data", default="all_data/results_JS-Kobori", help="‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
    parser.add_argument("--model", default="thai_model", help="‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•")
    
    args = parser.parse_args()
    
    pipeline = EasyOCRPipeline(args.data, args.model)
    success = pipeline.run_pipeline()
    
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()
